# noise test
model: gpt-3.5-turbo-0613 #gpt-3.5-turbo-0613 or llama2
dataset: family_relation # family_relation and base_math and GSM
start_num: 0
test_num: 200
run_times: 1
batch_size: 5

## in-context 
if_in_context: False
n_shots: 0 # self-made excellent shots
# n_weak_shots: 0 # original author's shots 

## noise
if_noise: False
n_noisy_shots: 5
noisy_type: irrelative # base math: irrelative or arithmetic_error
noisy_level: 3
## prefix
prefix_context: False

# math base
base_math:
  base: 9
  

# gpt
gpt:
  api: openai  # openai or hkbu
  temperature: 0

family_relation:
  train_set: 5
  reasoning_type: "symbolic"
  hop: 4

# llama2 
llama2:
  ckpt_dir: /vol/home/lanlong/xuanli/code/llama-main/llama-2-7b-chat/
  tokenizer_path: /vol/home/lanlong/xuanli/code/llama-main/tokenizer.model
  max_seq_len: 4000
  max_batch_size: 10
