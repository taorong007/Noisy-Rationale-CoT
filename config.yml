# noise test
model: gpt-3.5-turbo-0613
dataset: base_math # family_relation and base_math
start_num: 0
test_num: 200
run_times: 5
batch_size: 5

## in-context 
if_in_context: True
n_shots: 0 # self-made excellent shots
# n_weak_shots: 0 # original author's shots 

## noise
if_noise: True
n_noisy_shots: 3
noisy_type: irrelative # base math: irrelative or arithmetic_error
noisy_level: 3
## prefix
# prefix_context: True

# math base
base_math:
  base: 9
  

# GPT
GPT:
  api: openai  # openai or hkbu

family_relation:
  train_set: 3

# llama2 
llama2:
  ckpt_dir: /vol/home/lanlong/xuanli/code/llama-main/llama-2-7b-chat/
  tokenizer_path: /vol/home/lanlong/xuanli/code/llama-main/tokenizer.model
  max_seq_len: 4000
  max_batch_size: 10
