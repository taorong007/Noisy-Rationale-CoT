# noise test
model: gpt-3.5-turbo-0613 #gpt-3.5-turbo-0613 or llama2
dataset: SCAN # family_relation and base_math and GSM and SCAN
start_num: 0
test_num: 300
n_reason: 5
batch_size: 5
temperature_reason: 1

## in-context 
if_in_context: True
n_shots: 0 # self-made excellent shots
# n_weak_shots: 0 # original author's shots 

## noise
if_noise: True
n_noisy_shots: 3
noisy_type: irrelative # base math: irrelative or arithmetic_error
noisy_level: 3
## prefix
prefix_context: False

# method
## rephrase
if_rephrase: True
temperature_rephrase: 1
rephrase_aggregate: False # aggregate rephrase before ICL or after ICL
n_rephrase: 5 # only valid when rephrase_aggregate is ture

# math base
base_math:
  base: 9

# SCAN
SCAN:
  reasoning_type: "simple"
# gpt
gpt:
  api: openai  # openai or hkbu
  # api_base: "https://openkey.cloud/v1"


family_relation:
  train_set: 5
  reasoning_type: "symbolic"
  hop: 3

# llama2 
llama2:
  ckpt_dir: /vol/home/lanlong/xuanli/code/llama-main/llama-2-7b-chat/
  tokenizer_path: /vol/home/lanlong/xuanli/code/llama-main/tokenizer.model
  max_seq_len: 4000
  max_batch_size: 10
