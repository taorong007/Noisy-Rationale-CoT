# noise test
model: gpt-3.5-turbo-0613 #gpt-3.5-turbo-0613 or llama2
dataset: base_math # family_relation and base_math and GSM
start_num: 0
test_num: 300
batch_size: 1
temperature_rephrase: 1
n_rephrase: 5
temperature_reason: 0.1
n_reason: 5
rephrase_aggregate: True # aggregate rephrase before ICL or after ICL

## in-context 
if_in_context: True
n_shots: 0 # self-made excellent shots
# n_weak_shots: 0 # original author's shots 

## noise
if_noise: True
n_noisy_shots: 3
noisy_type: arithmetic_error # base math: irrelative or arithmetic_error
noisy_level: 3
## prefix
prefix_context: False

# math base
base_math:
  base: 9
  

# gpt
gpt:
  api: openai  # openai or hkbu


family_relation:
  train_set: 5
  reasoning_type: "symbolic"
  hop: 3

# llama2 
llama2:
  ckpt_dir: /vol/home/lanlong/xuanli/code/llama-main/llama-2-7b-chat/
  tokenizer_path: /vol/home/lanlong/xuanli/code/llama-main/tokenizer.model
  max_seq_len: 4000
  max_batch_size: 10
