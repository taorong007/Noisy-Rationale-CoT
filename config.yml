# noise test
model: mixtral # gpt-3.5-turbo-0613 or gemini-pro or mixtral or  llama-2-70b
dataset: base_math # family_relation and base_math and SCAN
start_num: 0
test_num: 300
batch_size: 1

# subtask
## base cal
base_math:
  reasoning_type: base-9

## SCAN
SCAN:
  reasoning_type: simple # simple or length

## family rel
family_relation:
  reasoning_type: symbolic
  # train_set: 5
  # hop: 3

# use subfolder_suffix
# subfolder_suffix_path: ablation_study

# use_processed_dataset or use raw dataset
use_processed_dataset: True

## when use_processed_dataset is True
processed_dataset_options:
  # processed dataset path or one of ["default-zeroshot"ï¼Œ "default-clean", "default-(irrelevant|inaccurate)-(easy|medium|hard)-(fixed|random)"]
  processed_dataset_path: default-irrelevant-medium-fixed
  n_shots: 3
  # using_subset: False
  # processed_dataset_path: default-zeroshot

# when use_processed_datset is False
# raw_dataset_options:
#   ## in-context 
#   if_in_context: True
#   n_shots: 0 # self-made excellent shots
#   # n_weak_shots: 0

#   ## noise
#   if_noise: True
#   n_noisy_shots: 10
#   noise_type: inaccurate # irrelevant or inaccurate
#   noise_ratio: 0.8 # 0.3, 0.5 or 0.8
#   noise_distribution: fixed #fixed or random

# ICL format
prefix_context: False ## prefix

# method
method: BT  # CD-CoT or baseline,  smoothllm, selfdenoise, selfpolish, contrastivecot, ISC, SCO, BT


## baseline, smoothllm, selfdenoise, selfpolish, contrastivecot, ISC
temperature_reason: 1
n_reason: 5

## CD-CoT
use_logged_rephrased_result: False
n_rephrase: 5
temperature_rephrase: 1
topp_rephrase: 1
m_select: 2 # 1 or 2 or 3 or 5
use_logged_ICL_result: False
use_clean_shot: True
# use_noisy_shot: False
c_reason: [3,2] # [5] or [3,2] or [2,2,1] or [1,1,1,1,1]
temp_reason: 1
topp_reason: 1

### RV
#RV_n_reason: 1
#RV_temp_reason: 0.1 # 0.1
#RV_topp_reason: 1 # 1
### RAV
#RAV_n_reason: 5
#RAV_temp_reason: 1 # 1
#RAV_topp_reason: 0.9 # 0.9

### both
#RV_weight: 0.5
#RAV_weight: 0.5

# model
## gpt
gpt:
  api: openai  # openai or hkbu
  # api_base: "https://openkey.cloud/v1"

## llama2 
# llama2:
#   ckpt_dir: /vol/home/lanlong/xuanli/code/llama-main/llama-2-7b-chat/
#   tokenizer_path: /vol/home/lanlong/xuanli/code/llama-main/tokenizer.model
#   max_seq_len: 4000
#   max_batch_size: 10
