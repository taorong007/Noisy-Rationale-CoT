# noise test
model: gpt-3.5-turbo-0613 #gpt-3.5-turbo-0613 or llama2
dataset: base_math # family_relation and base_math and GSM and SCAN
start_num: 0
test_num:  300
batch_size: 5

## in-context 
if_in_context: True
n_shots: 0 # self-made excellent shots
# n_weak_shots: 0 # original author's shots 
prefix_context: False ## prefix

## noise
if_noise: True
n_noisy_shots: 3
noise_type: minor_error # irrelevant or minor_error
noise_ratio: 0.8 # 0.3, 0.5 or 0.8
noise_distribution: fixed #fixed or random


# method
method: selfdenoise # RV or RAV or both or baseline,  smoothllm, selfdenoise, selfdenoise

## baseline, smoothllm
temperature_reason: 1
n_reason: 5

## RV or RAV or both
temperature_rephrase: 0.75
n_rephrase: 5

## RV
RV_n_reason: 1
RV_temp_reason: 1 # 0.1
RV_topp_reason: 0.9 # 1

## RAV
RAV_n_reason: 5
RAV_temp_reason: 1 # 1
RAV_topp_reason: 0.9 # 0.9

# both
RV_weight: 0.5
RAV_weight: 0.5


# math base
base_math:
  base: 9

# SCAN
SCAN:
  reasoning_type: "simple"

family_relation:
  train_set: 5
  reasoning_type: "symbolic"
  hop: 3

shuffled_obj:
  obj_num: 7

# gpt
gpt:
  api: openai  # openai or hkbu
  # api_base: "https://openkey.cloud/v1"

# llama2 
llama2:
  ckpt_dir: /vol/home/lanlong/xuanli/code/llama-main/llama-2-7b-chat/
  tokenizer_path: /vol/home/lanlong/xuanli/code/llama-main/tokenizer.model
  max_seq_len: 4000
  max_batch_size: 10

BBH:
  reasoning_type: formal_fallacies